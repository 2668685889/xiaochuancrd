#!/usr/bin/env python3
"""
æ™ºèƒ½åŠ©æ‰‹ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–è„šæœ¬
ç”¨äºåˆ†æå’Œä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ç“¶é¢ˆ
"""

import time
import requests
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
import statistics

def test_api_response_time(url, data=None, method='GET', headers=None):
    """æµ‹è¯•å•ä¸ªAPIæ¥å£çš„å“åº”æ—¶é—´"""
    start_time = time.time()
    try:
        if method == 'GET':
            response = requests.get(url, headers=headers, timeout=30)
        else:
            response = requests.post(url, json=data, headers=headers, timeout=30)
        
        end_time = time.time()
        response_time = end_time - start_time
        
        return {
            'url': url,
            'method': method,
            'status_code': response.status_code,
            'response_time': response_time,
            'success': response.status_code == 200,
            'response_size': len(response.content)
        }
    except Exception as e:
        end_time = time.time()
        return {
            'url': url,
            'method': method,
            'status_code': None,
            'response_time': end_time - start_time,
            'success': False,
            'error': str(e)
        }

def test_concurrent_requests(url, concurrent_requests=10):
    """æµ‹è¯•å¹¶å‘è¯·æ±‚æ€§èƒ½"""
    print(f"\nğŸ“Š æµ‹è¯•å¹¶å‘æ€§èƒ½: {concurrent_requests} ä¸ªå¹¶å‘è¯·æ±‚åˆ° {url}")
    
    start_time = time.time()
    response_times = []
    
    with ThreadPoolExecutor(max_workers=concurrent_requests) as executor:
        futures = [executor.submit(test_api_response_time, url) for _ in range(concurrent_requests)]
        
        for future in as_completed(futures):
            result = future.result()
            response_times.append(result['response_time'])
            
            if result['success']:
                print(f"  âœ… è¯·æ±‚æˆåŠŸ - å“åº”æ—¶é—´: {result['response_time']:.2f}ç§’")
            else:
                print(f"  âŒ è¯·æ±‚å¤±è´¥ - é”™è¯¯: {result.get('error', 'Unknown error')}")
    
    end_time = time.time()
    total_time = end_time - start_time
    
    return {
        'total_requests': concurrent_requests,
        'total_time': total_time,
        'avg_response_time': statistics.mean(response_times) if response_times else 0,
        'min_response_time': min(response_times) if response_times else 0,
        'max_response_time': max(response_times) if response_times else 0,
        'requests_per_second': concurrent_requests / total_time if total_time > 0 else 0
    }

def analyze_database_performance():
    """åˆ†ææ•°æ®åº“æŸ¥è¯¢æ€§èƒ½"""
    print("\nğŸ” åˆ†ææ•°æ®åº“æŸ¥è¯¢æ€§èƒ½...")
    
    # æµ‹è¯•ç®€å•çš„æ•°æ®åº“æŸ¥è¯¢
    simple_queries = [
        {
            'name': 'æŸ¥è¯¢äº§å“æ•°é‡',
            'url': 'http://localhost:8000/api/v1/smart-assistant/chat',
            'data': {'message': 'æŸ¥è¯¢äº§å“æ•°é‡', 'session_id': 'perf-test-001'}
        },
        {
            'name': 'æŸ¥è¯¢å®¢æˆ·ä¿¡æ¯',
            'url': 'http://localhost:8000/api/v1/smart-assistant/chat',
            'data': {'message': 'æŸ¥è¯¢å®¢æˆ·ä¿¡æ¯', 'session_id': 'perf-test-002'}
        },
        {
            'name': 'æŸ¥è¯¢é”€å”®è®¢å•',
            'url': 'http://localhost:8000/api/v1/smart-assistant/chat',
            'data': {'message': 'æŸ¥è¯¢é”€å”®è®¢å•', 'session_id': 'perf-test-003'}
        }
    ]
    
    results = []
    for query in simple_queries:
        result = test_api_response_time(query['url'], query['data'], 'POST')
        results.append({
            'query_name': query['name'],
            'response_time': result['response_time'],
            'success': result['success']
        })
        
        if result['success']:
            print(f"  âœ… {query['name']}: {result['response_time']:.2f}ç§’")
        else:
            print(f"  âŒ {query['name']}: å¤±è´¥ - {result.get('error', 'Unknown error')}")
    
    return results

def check_system_health():
    """æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€"""
    print("\nğŸ¥ æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€...")
    
    health_endpoints = [
        {'name': 'æ™ºèƒ½åŠ©æ‰‹ä¿¡æ¯', 'url': 'http://localhost:8000/api/v1/smart-assistant/info'},
        {'name': 'æ¨¡å‹é…ç½®', 'url': 'http://localhost:8000/api/v1/smart-assistant/model-config'},
        {'name': 'èŠå¤©å†å²', 'url': 'http://localhost:8000/api/v1/smart-assistant/history'},
        {'name': 'å‰ç«¯é¡µé¢', 'url': 'http://localhost:3001'}
    ]
    
    health_status = {}
    for endpoint in health_endpoints:
        result = test_api_response_time(endpoint['url'])
        health_status[endpoint['name']] = {
            'available': result['success'],
            'response_time': result['response_time']
        }
        
        status_icon = 'âœ…' if result['success'] else 'âŒ'
        print(f"  {status_icon} {endpoint['name']}: {'å¯ç”¨' if result['success'] else 'ä¸å¯ç”¨'} - {result['response_time']:.2f}ç§’")
    
    return health_status

def generate_optimization_recommendations(performance_data):
    """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
    print("\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
    
    recommendations = []
    
    # åˆ†æå“åº”æ—¶é—´
    avg_response_time = performance_data.get('avg_response_time', 0)
    if avg_response_time > 3:
        recommendations.append("ğŸ”´ é«˜ä¼˜å…ˆçº§: å¹³å‡å“åº”æ—¶é—´è¶…è¿‡3ç§’ï¼Œéœ€è¦ç«‹å³ä¼˜åŒ–")
        recommendations.append("   - æ£€æŸ¥æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ï¼Œæ·»åŠ ç´¢å¼•")
        recommendations.append("   - ä¼˜åŒ–SQLæŸ¥è¯¢è¯­å¥ï¼Œé¿å…å…¨è¡¨æ‰«æ")
        recommendations.append("   - è€ƒè™‘æ·»åŠ æŸ¥è¯¢ç¼“å­˜æœºåˆ¶")
    elif avg_response_time > 1:
        recommendations.append("ğŸŸ¡ ä¸­ä¼˜å…ˆçº§: å¹³å‡å“åº”æ—¶é—´è¶…è¿‡1ç§’ï¼Œå»ºè®®ä¼˜åŒ–")
        recommendations.append("   - æ£€æŸ¥APIæ¥å£çš„æ•°æ®åº“æŸ¥è¯¢")
        recommendations.append("   - ä¼˜åŒ–å‰ç«¯èµ„æºåŠ è½½")
        recommendations.append("   - è€ƒè™‘ä½¿ç”¨å¼‚æ­¥å¤„ç†")
    else:
        recommendations.append("ğŸŸ¢ è‰¯å¥½: å“åº”æ—¶é—´åœ¨å¯æ¥å—èŒƒå›´å†…")
    
    # åˆ†æå¹¶å‘æ€§èƒ½
    if performance_data.get('requests_per_second', 0) < 5:
        recommendations.append("ğŸ”´ é«˜ä¼˜å…ˆçº§: å¹¶å‘å¤„ç†èƒ½åŠ›ä¸è¶³")
        recommendations.append("   - æ£€æŸ¥æ•°æ®åº“è¿æ¥æ± é…ç½®")
        recommendations.append("   - ä¼˜åŒ–æœåŠ¡å™¨èµ„æºé…ç½®")
        recommendations.append("   - è€ƒè™‘è´Ÿè½½å‡è¡¡")
    
    # æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€
    health_status = performance_data.get('health_status', {})
    unavailable_services = [name for name, status in health_status.items() if not status['available']]
    if unavailable_services:
        recommendations.append(f"ğŸ”´ é«˜ä¼˜å…ˆçº§: ä»¥ä¸‹æœåŠ¡ä¸å¯ç”¨: {', '.join(unavailable_services)}")
    
    for rec in recommendations:
        print(rec)
    
    return recommendations

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ™ºèƒ½åŠ©æ‰‹ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–åˆ†æ")
    print("=" * 50)
    
    # 1. æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€
    health_status = check_system_health()
    
    # 2. æµ‹è¯•ä¸»è¦APIæ¥å£å“åº”æ—¶é—´
    print("\nâ±ï¸ æµ‹è¯•ä¸»è¦APIæ¥å£å“åº”æ—¶é—´...")
    
    api_endpoints = [
        'http://localhost:8000/api/v1/smart-assistant/info',
        'http://localhost:8000/api/v1/smart-assistant/model-config',
        'http://localhost:8000/api/v1/smart-assistant/history'
    ]
    
    api_results = []
    for endpoint in api_endpoints:
        result = test_api_response_time(endpoint)
        api_results.append(result)
        
        status_icon = 'âœ…' if result['success'] else 'âŒ'
        print(f"  {status_icon} {endpoint}: {result['response_time']:.2f}ç§’")
    
    # 3. æµ‹è¯•æ™ºèƒ½åŠ©æ‰‹èŠå¤©åŠŸèƒ½æ€§èƒ½
    print("\nğŸ¤– æµ‹è¯•æ™ºèƒ½åŠ©æ‰‹èŠå¤©åŠŸèƒ½æ€§èƒ½...")
    chat_results = analyze_database_performance()
    
    # 4. æµ‹è¯•å¹¶å‘æ€§èƒ½
    concurrent_results = test_concurrent_requests('http://localhost:8000/api/v1/smart-assistant/info', 5)
    
    # 5. æ±‡æ€»æ€§èƒ½æ•°æ®
    all_response_times = [r['response_time'] for r in api_results if r['success']]
    all_response_times.extend([r['response_time'] for r in chat_results if r['success']])
    
    performance_data = {
        'health_status': health_status,
        'api_results': api_results,
        'chat_results': chat_results,
        'concurrent_results': concurrent_results,
        'avg_response_time': statistics.mean(all_response_times) if all_response_times else 0,
        'total_tests': len(all_response_times)
    }
    
    # 6. ç”Ÿæˆä¼˜åŒ–å»ºè®®
    generate_optimization_recommendations(performance_data)
    
    # 7. è¾“å‡ºæ€§èƒ½æŠ¥å‘Š
    print("\nğŸ“Š æ€§èƒ½æŠ¥å‘Šæ‘˜è¦:")
    print(f"  å¹³å‡å“åº”æ—¶é—´: {performance_data['avg_response_time']:.2f}ç§’")
    print(f"  å¹¶å‘å¤„ç†èƒ½åŠ›: {concurrent_results['requests_per_second']:.1f} è¯·æ±‚/ç§’")
    print(f"  æ€»æµ‹è¯•æ¬¡æ•°: {performance_data['total_tests']}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸¥é‡é—®é¢˜
    if performance_data['avg_response_time'] > 5:
        print("\nâš ï¸ è­¦å‘Š: ç³»ç»Ÿæ€§èƒ½è¾ƒå·®ï¼Œéœ€è¦ç«‹å³ä¼˜åŒ–ï¼")
    elif performance_data['avg_response_time'] > 2:
        print("\nâ„¹ï¸ æç¤º: ç³»ç»Ÿæ€§èƒ½æœ‰å¾…ä¼˜åŒ–")
    else:
        print("\nâœ… ç³»ç»Ÿæ€§èƒ½è‰¯å¥½")

if __name__ == "__main__":
    main()